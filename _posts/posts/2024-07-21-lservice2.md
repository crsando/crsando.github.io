---
layout: post
title: 关于lservice与lctpc的重构
tags: trading,weekly
category: posts
---

# background

国内期货程序化交易的基础是上海期货证券公司的CTP。个人投资者通过券商申请程序化结构，通过自行编程或者其它人使用的CTP接入工具，按照券商提供的接口进行接入。

CTP实际上是一个C++编写的库，可以理解成它自己有一个基于流的与服务器的通讯，调用方法主要是用各种Req开头的Request方法发出请求，之后在C++中重载各种OnRsp<XXXX>方法来处理返回的信息。（讲实话，这种方式真的挺繁琐且不现代的）

其实在tifa.lua，lservice和lctpc项目之前，最早我用基于ctp-python（其实是用SWIG，做了一个对CTP的封装）的方式在python里写了一版框架。但是为什么后来不做下去了，也是在做的过程中意识到了一系列的事情：

    1. 我对于这种异步IO/多线程/并行编程的认知和经验还是有些不够，纯靠调包的话其实不是很理想。
    2. 虽然Python很好，但是我还是很怀念用Lua。有的时候觉得人也不用太功利，尤其是个人开发可以选择自己喜欢的技术路线。
    3. 即便用Python（包括后来改到Lua）始终面对一个问题：即使用很多异步IO框架和库，但是任何把CTP这样的第三方库和现有的模式进行兼容整合？

# choice of luajit

这里的问题2，其实后来演变成了对于luajit的选择。实际上正如同CTP是C++开发的，选择luajit相比于python的一个显著的好处就是方便的绑定C/C++的代码，只要做一层C API的bing，然后就可以直接用luajit的ffi方便的做lua-binding了。而且这个过程中，一方面确实考虑到最后性能优先的部分总是要换成C语言实现的，而lua也是我比较熟悉，开发方便。Python可以放在做数据分析的过程中，策略部分的代码反正估计也不会太复杂才对。

其实曾经为了练手，用lua的标准方式做了一些CTP的binding，只能说相比于luajit来说实在是太累了。所以被放弃了。

后来基于luajit的binding就是lctp和lctp2。这两者的区别是前者是延续CTP的回调函数模式，基于luajit自动把lua function转成c function，然后在CTP中处理回调。lctp2则是取消了这点，换成了一套类似push/pull或者说query/fetch的思路：一个lua函数发出请求，一个lua函数来收取消息结果。这也是luajit推荐的方式。这其中的一系列问题在下面会陆续提到。

# concurrency programming

其实问题3最后看下来是最重要和复杂的。比如在Python中的asycnio, 比如Node.js的一系列东西（包括对于底层的libuv的一些研究），也包括后来在Lua里尝试了luv（一个对libuv的封装）。

其实现代这种异步IO并行来说，async/await已经成为事实上的标准，虽然一开始非常难理解但是后来看懂了确实还是最方便的方式。但是这些和CTP的这种C++回调函数的模式感觉找不到兼容的方法，研究了很长一段时间仍然无果。

后来感觉让自己提升比较大的是去看了云飞大神（cloudwu）的skynet/ltask，虽然过程很痛苦（文档真的是看不懂），但是啃下来感觉对这种异步编程的理解上了一个维度了。其实最初看的感觉就觉得这东西的很多思路和我需要的非常接近，只是文档确实没看懂。后来花了一些时间研究了ltask的代码，才算有了领悟。

# a thought on await/async and lua coroutine

我觉得与其空说不如结合具体一点的，来讲一下对于CTP这种模式，await/async，以及Lua Coroutine的理解。

假设我们向服务器查询期货账户的持仓，对于CTP来说是发送一个`ReqQryInvestorPosition`，之后通过重载`OnRspQryInvestorPosition`，来处理返回结果。返回是一条一条返回的，就是每次`OnRspQryInvestorPosition`都是处理一条。

这里有两个问题：

1. 在一个执行序列中，如果先发送request，等待结果返回，然后再执行下一个动作（比如发出另一个request）
2. 多条返回结果如何组合在一起？

先说1，其实从Node.js/Libuv的角度来说，大概相对于`let r = await request_xxx`，这个await的过程中，当前的执行序列会被挂起，转而执行其它任务，直到结果返回，重新回到这个执行流中，给`r`赋值，然后进行下一条。这一条`await`，需要被包裹在一个`async`函数当中，才能正确执行。一个`async`函数（的调用），可以理解为启动了一条单独的执行流，每个执行流独立，且每个执行流可以被挂起，来避免IO阻塞占用CPU时间。

对应到Lua的coroutine来说，可以当作是一个async函数的调用是一个coroutine，每次启动一个coroutine,到了`await`的时候，其实就是`coroutine.yield()`，当前coroutine被挂起，直到我们得到了IO结果后，通过外部来执行`coroutine.resume(co, ...)`

大概这个感觉

```lua
local co = coroutine.create(function ()
    request() -- non-blocking
    local rst = coroutine.yield()
    return rst
end)
```

在skynet和ltask中大概就是这么一个思路，在Lua中实现了Actor Model。我粗略一点的思路去理解，大概就是同时存在许多个service（每个service有自己独立的lua vm），每个service有一个message queue，不同service之间通过消息沟通，或者更粗略的说，就是可以互相call来call去。一个call就是一个request加一个response。（是不是这里就和web app很像了）。

比如拿ltask测试案例中来说，假设我们实现一个`user.lua`的service，其中实现了一个`ping`方法

```lua
local S = {}

function S.ping()
    return "PONG"
end

return S
```

那么当我们从另一个服务（比如root），就可以`ltask.call(user, "ping")`, 来实现类似一个Remote Procedure Call的效果。

这个调用从底层上，大概捋一下逻辑，是这样的：

1. 一个`ltask.call`会启动一个新的session（有一个新的session_id），一个session对应被call这里的一个单独的coroutine。
2. 被call的服务上有很多coroutine同时存在, 这些coroutine（每个coroutine对应一个session）可以认为是并行的。
3. 主call端也必须是一个服务，其本身处在主call服务的某一个session中，这个session在call发出后即挂起(`coroutine.yield`)
4. 被call这边，比如我们的`ping`，在`return`后，会由session的相关逻辑，自动的`send_response`给主call服务
5. 发出call和接受返回都是通过消息传递，消息类型不同（分别为`MESSAGE_REQUEST`和`MESSAGE_RESPONE`），以`session_id`来识别消息对应的session, 通过`lua-seri`这个模块来对lua object进行序列化和反序列化。
6. 一个服务其实就是轮询（实际上更复杂一点）来处理消息，核心消息处理逻辑在`ltask.schedule_message`（定义在`service.lua`中）
7. 被call端接收到了`MESSAGE_REQUEST`类型的消息，匹配handler，然后通过`new_thread`和`new_session`来启动一个session（e.g. 绑定对应的`S.ping`，启动一个新的coroutine）
8. 主call端接收到了`MESSAGE_REPONSE`类型的消息后，通过`resume_session`（实际上就是`coroutine.resume`）重新启动之前被挂起的session，并这条response的信息丢过去，从效果上来看，就是这个session发出call后被挂起，收到结果后重新顺序执行下去。

这里面还有很多细节。比如消息发出后有一个RECEIPT，错误处理和traceback等，不得不说云风大神写的真的是非常NB和完善。

# lservice2

其实当时看的感觉就是这套skynet/ltask的Actor Model思路非常适合用来做quant system，在一个这样的系统中，可能有多个Trading Bot以及一系列基础组件。比如对应CTP来说，trader是一个单独服务，每个bot都可以call trader来下单。然后接受数据tick也是一个单独的服务，
在接受到tick数据后，广播给每个bot，每个bot接受到tick之后开始执行对应的strategy逻辑过程。

所以就有了自己开发的简单的lservice项目，初衷是：

1. skynet/ltask都是基于Lua5.4的api，而luajit还是5.1的api，没办法编译上luajit（当然，后来我通过改代码成功了，放在附录中）
2. skynet/ltask的模式对于我的需求来说可能还是复杂了一点，拿ltask来说，它实际上是自己实现了一套任务调动，每个service都是一个单独的coroutine，可以被yield，然后通过worker模型（也就是介绍里写的，M个lua vm跑在N个thread上）。我后来选择的简单方法是每个service就跑在一个单独的thread上。
3. 自己写一遍代码更便于学习和研究理解。

实际上写的过程中也一直在参考ltask。最初的lservice第一版非常简陋和简单，本质就是启动一个新的thread来跑一个新的lua vm，同时提供一个消息队列，然后每次队列里有消息的时候，就call这个lua vm里的函数。这样每个lua vm其实就是类似于编写一个module，最后返回我们需要的消息处理函数即可。这个版本的过程中没有任何的coroutine的出现。

后来在持续研究的过程中终于搞明白上面关于await/async和coroutine的这些东西，慢慢能看懂ltask的代码在干些什么了，然后参考（copy）的重新了第二版的lservice2。

另一个点是对于ltask，我觉得service.lua的这套过程实在是过于繁琐了（详细的讨论放在附录中）。所以在写lservice2的时候，我尽量按照常规lua的require一个模块的方式来执行。

最终结果是，一个lservice2的serivce, 大概是一个这样的一段lua代码

```lua
local service = require "lservice2"
service.input(...)

--- do something
--- ...

local S = {}

function S.ping(say)
    print(say)
    return "PONG"
end

service.dispatch(S)

```

一个service可以通过`service.spawn{ source = "@service/user.lua", config = {...}}`来启动，实际非常简单，就是新建了一个thread和一个lua vm，并且在其中执行`source`指向的代码（这里的`@`是沿用了`ltask`的规则，带`@`表示加载代码文件，不带`@`代表直接的代码）.

在`require "lservice2"`，我们加载了所有的必要的方法和过程，实现了逻辑封装。

这个lua vm在运行代码的过程中，通过`...`传递了两个参数`(ptr, config)`，前者是一个lightuserdata，指向这个服务本身（底层C代码是一个`struct service`）, 后者就是我们在`spawn`时候指定的`config`。

通过`service.input(...)`，实现了对于一些必要基础的安排，比如把`ptr`保存起来（这样在后续的执行中，就能通过`ptr`找到自己的服务等等）

最后的`service.dispatch(S)`启动了消息循环，通过阻塞的方式监听服务自己的消息队列。背后的实现是conditional variable。

# combine lctp2 and lservice2

现在谈谈怎么把CTP的东西和lservice2整合在一起。问题是什么呢？其实最主要的问题是我们虽然有多线程功能，但是一段业务逻辑其实是希望单线程执行的。

Luajit不是thread-safe的，所以其实之前说的回调函数的模型，在CTP的设计上，请求`ReqQryXX`是在A线程上，返回报文的`OnRspXXX`是执行在B线程上。

我最后的设计思路是这样的，以实现一个trader service为例：

1. 处理消息和各种业务逻辑只在service所处的线程上继续进行。
2. 我们用query/fetch或者说send/recv的思路，设计lctp2的api，具体来说，关键是通过重载`OnRspXXX`，讲返回报文发送到trader自己的一个单独的message queue里头。这样，trader通过recv的方式可以从这个mq中取到结果。这个过程是完全在lctp2中进行的封装。
3. 然后，关键的结合点：我们通过一个hook，把service对应的conditional variable的指针给到lctp2部分，让上述返回报文发送给trader自己的mq的过程的同时，给service也发送一个信号（只signal conditional variable，不给service的消息队列发送消息）
4. 上述信号已经足够接触service的阻塞了。原本service的逻辑，就是不断的等待信号->处理消息->继续等待。现在变成了等待信号，如果有消息先处理消息，即使没有消息，也执行一个单独的idle函数。
5. 在idle函数里头，我们编写处理`OnRspXXX`的业务规则和逻辑。
6. 这里我们做个假设约束：一个对于CTP的query/request，一定是在某一个service的session中进行的，因为是在一个session中，我们在发出这个request后挂起这个session，同时在一个table中记录一下（session, req_id）这个组合（这个req_id对应我们CTP请求的request_id，我们在封装中把它做成自增，不重复的，唯一标识）。
7. 因为上述session被挂起，所以此时可以接受并处理其它消息（也就是运行其它session）
8. 在idle函数中，我们处理`OnRspXXX`的业务逻辑，如果我们看到一个req_id的请求已经完成了，收集所有请求的结果，resume对应的session，并讲结果抛过去。

这样两组不同的消息（一组来自quant system内部，不同service之间的消息互传，一组是来自service与CTP之间的通信）就通过共享conditional variable和session的方式整合到了一起了。这样的效果是，在CTP request的过程中，service可以临时挂起去处理其它的系统消息。在处理系统消息的间隙，又可以处理CTP response。

# 研究 ltask

这里准备后续记录一些ltask的研究，对我来说代码不是很好懂，可能有些地方比较反直觉。还有一些是在ltask改写道luajit上的一些情况

未完待续